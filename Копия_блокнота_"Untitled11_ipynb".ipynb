{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMeh2JmSTVhWcnA879w/oqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seludkoaleksandr95-coder/Nto/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22Untitled11_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/stage1_individual_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ-lRN3sIpWA",
        "outputId": "4d0898fb-39c1-4542-a766-75dc3d0d72c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/stage1_individual_data.zip\n",
            "  inflating: book_descriptions.csv   \n",
            "  inflating: book_genres.csv         \n",
            "  inflating: books.csv               \n",
            "  inflating: genres.csv              \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n",
            "  inflating: users.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import joblib\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "if 'has_read' in train.columns:\n",
        "    train = train[train['has_read'] == 1].copy()\n",
        "\n",
        "y_original = train['rating'].values\n",
        "global_mean = train['rating'].mean()\n",
        "\n",
        "user_stats = train.groupby('user_id')['rating'].agg([\n",
        "    'mean', 'std', 'count', 'min', 'max', 'median',\n",
        "    lambda x: (x == 10).sum(),\n",
        "    lambda x: (x >= 8).sum(),\n",
        "    lambda x: (x <= 5).sum(),\n",
        "    lambda x: x.skew() if len(x) > 2 else 0\n",
        "]).reset_index()\n",
        "\n",
        "user_stats.columns = [\n",
        "    'user_id', 'user_mean', 'user_std', 'user_count',\n",
        "    'user_min', 'user_max', 'user_median', 'user_count_10',\n",
        "    'user_count_high', 'user_count_low', 'user_skew'\n",
        "]\n",
        "\n",
        "user_stats['user_10_ratio'] = user_stats['user_count_10'] / user_stats['user_count']\n",
        "user_stats['user_high_ratio'] = user_stats['user_count_high'] / user_stats['user_count']\n",
        "user_stats['user_low_ratio'] = user_stats['user_count_low'] / user_stats['user_count']\n",
        "\n",
        "book_stats = train.groupby('book_id')['rating'].agg([\n",
        "    'mean', 'std', 'count', 'min', 'max', 'median',\n",
        "    lambda x: (x == 10).sum(),\n",
        "    lambda x: (x >= 8).sum(),\n",
        "    lambda x: (x <= 5).sum(),\n",
        "    lambda x: x.skew() if len(x) > 2 else 0\n",
        "]).reset_index()\n",
        "\n",
        "book_stats.columns = [\n",
        "    'book_id', 'book_mean', 'book_std', 'book_count',\n",
        "    'book_min', 'book_max', 'book_median', 'book_count_10',\n",
        "    'book_count_high', 'book_count_low', 'book_skew'\n",
        "]\n",
        "\n",
        "book_stats['book_10_ratio'] = book_stats['book_count_10'] / book_stats['book_count']\n",
        "book_stats['book_high_ratio'] = book_stats['book_count_high'] / book_stats['book_count']\n",
        "book_stats['book_low_ratio'] = book_stats['book_count_low'] / book_stats['book_count']\n",
        "\n",
        "if 'timestamp' in train.columns:\n",
        "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
        "    train['year'] = train['timestamp'].dt.year\n",
        "    train['month'] = train['timestamp'].dt.month\n",
        "    train['day'] = train['timestamp'].dt.day\n",
        "    train['dayofweek'] = train['timestamp'].dt.dayofweek\n",
        "    train['hour'] = train['timestamp'].dt.hour\n",
        "\n",
        "    train['month_sin'] = np.sin(2 * np.pi * train['month'] / 12)\n",
        "    train['month_cos'] = np.cos(2 * np.pi * train['month'] / 12)\n",
        "    train['dayofweek_sin'] = np.sin(2 * np.pi * train['dayofweek'] / 7)\n",
        "    train['dayofweek_cos'] = np.cos(2 * np.pi * train['dayofweek'] / 7)\n",
        "    train['hour_sin'] = np.sin(2 * np.pi * train['hour'] / 24)\n",
        "    train['hour_cos'] = np.cos(2 * np.pi * train['hour'] / 24)\n",
        "\n",
        "X_data = train[['user_id', 'book_id']].copy()\n",
        "if 'timestamp' in train.columns:\n",
        "    time_features = ['year', 'month', 'day', 'dayofweek', 'hour',\n",
        "                     'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
        "                     'hour_sin', 'hour_cos']\n",
        "    for feat in time_features:\n",
        "        X_data[feat] = train[feat]\n",
        "\n",
        "X_data = X_data.merge(user_stats, on='user_id', how='left')\n",
        "X_data = X_data.merge(book_stats, on='book_id', how='left')\n",
        "\n",
        "X_data['mean_diff'] = X_data['user_mean'] - X_data['book_mean']\n",
        "X_data['mean_avg'] = (X_data['user_mean'] + X_data['book_mean']) / 2\n",
        "X_data['std_avg'] = (X_data['user_std'] + X_data['book_std']) / 2\n",
        "X_data['count_ratio'] = np.log1p(X_data['user_count']) / (np.log1p(X_data['book_count']) + 1)\n",
        "X_data['10_ratio_diff'] = X_data['user_10_ratio'] - X_data['book_10_ratio']\n",
        "X_data['high_ratio_diff'] = X_data['user_high_ratio'] - X_data['book_high_ratio']\n",
        "X_data['low_ratio_diff'] = X_data['user_low_ratio'] - X_data['book_low_ratio']\n",
        "\n",
        "X_data['user_confidence'] = 1 - np.exp(-X_data['user_count'] / 10)\n",
        "X_data['book_confidence'] = 1 - np.exp(-X_data['book_count'] / 5)\n",
        "X_data['combined_confidence'] = (X_data['user_confidence'] + X_data['book_confidence']) / 2\n",
        "\n",
        "X_data['weighted_mean'] = (\n",
        "    X_data['user_mean'] * X_data['user_confidence'] +\n",
        "    X_data['book_mean'] * X_data['book_confidence']\n",
        ") / (X_data['user_confidence'] + X_data['book_confidence'] + 1e-10)\n",
        "\n",
        "for col in X_data.columns:\n",
        "    if col not in ['user_id', 'book_id']:\n",
        "        if col.endswith('_mean') or col.endswith('_median'):\n",
        "            X_data[col] = X_data[col].fillna(global_mean)\n",
        "        elif col.endswith('_count'):\n",
        "            X_data[col] = X_data[col].fillna(0)\n",
        "        elif col.endswith('_std') or col.endswith('_skew'):\n",
        "            X_data[col] = X_data[col].fillna(0)\n",
        "        elif col.endswith('_ratio') or 'confidence' in col:\n",
        "            X_data[col] = X_data[col].fillna(0)\n",
        "        else:\n",
        "            X_data[col] = X_data[col].fillna(0)\n",
        "\n",
        "feature_cols = [col for col in X_data.columns if col not in ['user_id', 'book_id']]\n",
        "\n",
        "X = X_data[feature_cols]\n",
        "y = train['rating'].values\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "models_predictions = {}\n",
        "models = {}\n",
        "\n",
        "lgb_predictions = np.zeros(len(X))\n",
        "lgb_models = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        max_depth=8,\n",
        "        min_child_samples=20,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=42 + fold,\n",
        "        verbose=-1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_val)\n",
        "    lgb_predictions[val_idx] = pred\n",
        "    lgb_models.append(model)\n",
        "\n",
        "lgb_rmse = np.sqrt(mean_squared_error(y, lgb_predictions))\n",
        "models['lgb'] = lgb_models\n",
        "models_predictions['lgb'] = lgb_predictions\n",
        "\n",
        "xgb_predictions = np.zeros(len(X))\n",
        "xgb_models = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=7,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=42 + fold,\n",
        "        n_jobs=-1,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_val)\n",
        "    xgb_predictions[val_idx] = pred\n",
        "    xgb_models.append(model)\n",
        "\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y, xgb_predictions))\n",
        "models['xgb'] = xgb_models\n",
        "models_predictions['xgb'] = xgb_predictions\n",
        "\n",
        "rf_predictions = np.zeros(len(X))\n",
        "rf_models = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        max_features=0.5,\n",
        "        random_state=42 + fold,\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_val)\n",
        "    rf_predictions[val_idx] = pred\n",
        "    rf_models.append(model)\n",
        "\n",
        "rf_rmse = np.sqrt(mean_squared_error(y, rf_predictions))\n",
        "models['rf'] = rf_models\n",
        "models_predictions['rf'] = rf_predictions\n",
        "\n",
        "user_mean_pred = X_data['user_mean'].fillna(global_mean).values\n",
        "user_mean_rmse = np.sqrt(mean_squared_error(y, user_mean_pred))\n",
        "models_predictions['user_mean'] = user_mean_pred\n",
        "\n",
        "book_mean_pred = X_data['book_mean'].fillna(global_mean).values\n",
        "book_mean_rmse = np.sqrt(mean_squared_error(y, book_mean_pred))\n",
        "models_predictions['book_mean'] = book_mean_pred\n",
        "\n",
        "weighted_pred = (\n",
        "    user_mean_pred * X_data['user_confidence'] +\n",
        "    book_mean_pred * X_data['book_confidence']\n",
        ") / (X_data['user_confidence'] + X_data['book_confidence'] + 1e-10)\n",
        "weighted_rmse = np.sqrt(mean_squared_error(y, weighted_pred))\n",
        "models_predictions['weighted'] = weighted_pred\n",
        "\n",
        "all_predictions = pd.DataFrame(models_predictions)\n",
        "\n",
        "def objective(weights):\n",
        "    weights = np.clip(weights, 0, 1)\n",
        "    weights = weights / weights.sum()\n",
        "\n",
        "    ensemble_pred = np.zeros(len(y))\n",
        "    for i, col in enumerate(all_predictions.columns):\n",
        "        ensemble_pred += all_predictions[col].values * weights[i]\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y, ensemble_pred))\n",
        "    return rmse\n",
        "\n",
        "initial_weights = np.ones(len(all_predictions.columns))\n",
        "for i, col in enumerate(all_predictions.columns):\n",
        "    if col == 'lgb':\n",
        "        initial_weights[i] = 3.0\n",
        "    elif col == 'xgb':\n",
        "        initial_weights[i] = 2.0\n",
        "    elif col == 'rf':\n",
        "        initial_weights[i] = 1.5\n",
        "    else:\n",
        "        initial_weights[i] = 0.5\n",
        "\n",
        "initial_weights = initial_weights / initial_weights.sum()\n",
        "\n",
        "bounds = [(0, 1) for _ in range(len(all_predictions.columns))]\n",
        "\n",
        "result = minimize(\n",
        "    objective,\n",
        "    initial_weights,\n",
        "    bounds=bounds,\n",
        "    method='SLSQP',\n",
        "    options={'maxiter': 100, 'disp': False}\n",
        ")\n",
        "\n",
        "optimal_weights = np.clip(result.x, 0, 1)\n",
        "optimal_weights = optimal_weights / optimal_weights.sum()\n",
        "\n",
        "ensemble_oof = np.zeros(len(y))\n",
        "for i, col in enumerate(all_predictions.columns):\n",
        "    ensemble_oof += all_predictions[col].values * optimal_weights[i]\n",
        "\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y, ensemble_oof))\n",
        "\n",
        "X_test = test[['user_id', 'book_id']].copy()\n",
        "\n",
        "if 'timestamp' in train.columns:\n",
        "    for feat in time_features:\n",
        "        X_test[feat] = train[feat].mean()\n",
        "\n",
        "X_test = X_test.merge(user_stats, on='user_id', how='left')\n",
        "X_test = X_test.merge(book_stats, on='book_id', how='left')\n",
        "\n",
        "X_test['mean_diff'] = X_test['user_mean'] - X_test['book_mean']\n",
        "X_test['mean_avg'] = (X_test['user_mean'] + X_test['book_mean']) / 2\n",
        "X_test['std_avg'] = (X_test['user_std'] + X_test['book_std']) / 2\n",
        "X_test['count_ratio'] = np.log1p(X_test['user_count']) / (np.log1p(X_test['book_count']) + 1)\n",
        "X_test['10_ratio_diff'] = X_test['user_10_ratio'] - X_test['book_10_ratio']\n",
        "X_test['high_ratio_diff'] = X_test['user_high_ratio'] - X_test['book_high_ratio']\n",
        "X_test['low_ratio_diff'] = X_test['user_low_ratio'] - X_test['book_low_ratio']\n",
        "\n",
        "X_test['user_confidence'] = 1 - np.exp(-X_test['user_count'] / 10)\n",
        "X_test['book_confidence'] = 1 - np.exp(-X_test['book_count'] / 5)\n",
        "X_test['combined_confidence'] = (X_test['user_confidence'] + X_test['book_confidence']) / 2\n",
        "\n",
        "X_test['weighted_mean'] = (\n",
        "    X_test['user_mean'] * X_test['user_confidence'] +\n",
        "    X_test['book_mean'] * X_test['book_confidence']\n",
        ") / (X_test['user_confidence'] + X_test['book_confidence'] + 1e-10)\n",
        "\n",
        "for col in feature_cols:\n",
        "    if col in X_test.columns:\n",
        "        if col.endswith('_mean') or col.endswith('_median'):\n",
        "            X_test[col] = X_test[col].fillna(global_mean)\n",
        "        elif col.endswith('_count'):\n",
        "            X_test[col] = X_test[col].fillna(0)\n",
        "        elif col.endswith('_std') or col.endswith('_skew'):\n",
        "            X_test[col] = X_test[col].fillna(0)\n",
        "        elif col.endswith('_ratio') or 'confidence' in col:\n",
        "            X_test[col] = X_test[col].fillna(0)\n",
        "        else:\n",
        "            X_test[col] = X_test[col].fillna(0)\n",
        "\n",
        "X_test_features = X_test[feature_cols]\n",
        "\n",
        "test_predictions_all = {}\n",
        "\n",
        "for name, model_list in models.items():\n",
        "    if name in ['lgb', 'xgb', 'rf']:\n",
        "        preds = np.zeros((len(X_test_features), len(model_list)))\n",
        "        for i, model in enumerate(model_list):\n",
        "            preds[:, i] = model.predict(X_test_features)\n",
        "        test_predictions_all[name] = preds.mean(axis=1)\n",
        "\n",
        "test_predictions_all['user_mean'] = X_test['user_mean'].fillna(global_mean).values\n",
        "test_predictions_all['book_mean'] = X_test['book_mean'].fillna(global_mean).values\n",
        "test_predictions_all['weighted'] = (\n",
        "    test_predictions_all['user_mean'] * X_test['user_confidence'] +\n",
        "    test_predictions_all['book_mean'] * X_test['book_confidence']\n",
        ") / (X_test['user_confidence'] + X_test['book_confidence'] + 1e-10)\n",
        "\n",
        "test_ensemble = np.zeros(len(X_test_features))\n",
        "for i, col in enumerate(all_predictions.columns):\n",
        "    if col in test_predictions_all:\n",
        "        test_ensemble += test_predictions_all[col] * optimal_weights[i]\n",
        "\n",
        "test_ensemble = np.clip(test_ensemble, 0, 10)\n",
        "\n",
        "target_distribution = dict(train['rating'].value_counts(normalize=True).sort_index())\n",
        "\n",
        "def adjust_distribution(preds, target_dist):\n",
        "    sorted_idx = np.argsort(preds)\n",
        "    sorted_preds = preds[sorted_idx]\n",
        "\n",
        "    n = len(preds)\n",
        "    adjusted = np.zeros_like(preds)\n",
        "\n",
        "    percentiles = np.cumsum(list(target_dist.values()))\n",
        "    percentiles = percentiles / percentiles[-1]\n",
        "\n",
        "    values = list(target_dist.keys())\n",
        "\n",
        "    for i, idx in enumerate(sorted_idx):\n",
        "        percentile = i / n\n",
        "        for j, p in enumerate(percentiles):\n",
        "            if percentile <= p:\n",
        "                adjusted[idx] = values[j]\n",
        "                break\n",
        "\n",
        "    return adjusted\n",
        "\n",
        "test_adjusted = adjust_distribution(test_ensemble, target_distribution)\n",
        "\n",
        "current_mean = test_adjusted.mean()\n",
        "if abs(current_mean - global_mean) > 0.1:\n",
        "    adjustment = (global_mean - current_mean) * 0.5\n",
        "    test_adjusted = np.clip(test_adjusted + adjustment, 0, 10)\n",
        "    test_adjusted = np.round(test_adjusted).astype(int)\n",
        "\n",
        "strategies = {}\n",
        "strategies['smart_dist'] = test_adjusted.copy()\n",
        "strategies['simple_round'] = np.round(test_ensemble)\n",
        "strategies['simple_round'] = np.clip(strategies['simple_round'], 0, 10).astype(int)\n",
        "\n",
        "submission_main = test[['user_id', 'book_id']].copy()\n",
        "submission_main['rating_predict'] = strategies['smart_dist']\n",
        "submission_main.to_csv('submission_ensemble.csv', index=False, float_format='%.1f')\n",
        "\n",
        "joblib.dump(models, 'ensemble_models.pkl')\n",
        "joblib.dump(feature_cols, 'feature_cols.pkl')\n",
        "joblib.dump(optimal_weights, 'optimal_weights.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByZgjsnMdoIk",
        "outputId": "779810ea-b862-41a1-ee33-932f3542f279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['optimal_weights.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}